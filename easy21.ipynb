{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easy 21 Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to implement a Monte Carlo learning agent and a SARSA agent to play Easy 21, which is a modified version of blackjack. The rules for Easy 21 are as follows:\n",
    "- Red and Black cards ranging from 1-10 in value\n",
    "- Black cards are added to score, Red cards are subtracted from score\n",
    "- Black cards occur with 2/3 probability, Red cards with 1/3 probability\n",
    "- Agent and Dealer each start with 1 black card (agent can see dealer's card)\n",
    "- Player can hit or stick on turn, once player sticks turn is over\n",
    "- Agent goes first, then dealer\n",
    "- Player with high score wins (bust if score > 21 or < 1)\n",
    "\n",
    "(from: https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "First we need to set up our Agent class, which will play Easy 21 against a \"dealer\" who hits if he has between 1 and 17 and sticks otherwise. We can use the Agent class to run both the Monte Carlo simulation and the SARSA simulation.\n",
    "The 3 actions we need to define for our Agent are:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**step()** - the agent either hits and is dealt a card, or sticks and we play out the remainder of the dealer's turn  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**choose_action()** - follow the current policy with probability 1 - $\\epsilon$ and explore (hit or stick randomly) with probability $\\epsilon$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**value_function()** - compute the best value for each possible state (agent total, dealer card)  \n",
    "  \n",
    "We also set up 3 functions to facilitate gameplay:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**deal_card()** - Returns a black card with a random value between 1 and 10 with probability 2/3 and returns a red card with a random value between 1 and &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;10 with probability 1/3  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**is_terminal()** - Evaluates whether the agent or dealer has gone bust, or whether the agent has chosen to stick and the dealer has 17 or higher\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**give_reward()** - Gives the agent a reward of 1 for a win, 0 for a non-terminal state or tie, and -1 for a loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, epsilon):\n",
    "        '''\n",
    "        params:\n",
    "                epsilon - exploration parameter\n",
    "                stick - boolean used to help determine if state is terminal\n",
    "                values - action/value approximation\n",
    "                running_total - agent score, used to calculate statistics for training runs\n",
    "                policy - agent's preference (hit/stick) in each state (agent sum, dealer card)\n",
    "        '''\n",
    "        self.epsilon = epsilon\n",
    "        self.stick = False\n",
    "        self.values = np.zeros((21, 10, 2))\n",
    "        self.running_total = [0]\n",
    "        self.policy = [[0 for i in range(10)] for i in range(21)]\n",
    "        \n",
    "    def step(self, state, action):\n",
    "        '''\n",
    "        If player hits (0), deal card.  \n",
    "        If player sticks (1), play out the dealer's turn\n",
    "        '''\n",
    "        if action == 0:\n",
    "            state[0] += deal_card(False)\n",
    "        else:\n",
    "            self.stick = True\n",
    "            while state[1] < 17 and state[1] >= 1:\n",
    "                state[1] += deal_card(False)\n",
    "\n",
    "        return self.give_reward(state)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        greedy = np.random.random()\n",
    "        if greedy > self.epsilon:\n",
    "            action = self.policy[state[0] - 1][state[1] - 1]\n",
    "        else:\n",
    "            action = np.random.randint(2)\n",
    "        return action\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        '''\n",
    "        Returns true if game is over (both players stick or one player goes bust), false otherwise\n",
    "        '''\n",
    "        if state[0] > 21 or state[0]< 1:\n",
    "            return True\n",
    "\n",
    "        elif self.stick and (state[1] >= 17 or state[1] < 1):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "    \n",
    "    def give_reward(self, state):\n",
    "        '''\n",
    "        Reward = +1 (win), -1 (loss), 0 (tie or non-terminal state)\n",
    "        '''\n",
    "        if self.is_terminal(state):\n",
    "            if state[0] > 21 or state[0] < 1:\n",
    "                self.running_total.append((self.running_total[-1] - 1))\n",
    "                return -1\n",
    "            elif state[1] > 21 or state[1] < 1:\n",
    "                self.running_total.append((self.running_total[-1] + 1))\n",
    "                return 1\n",
    "            elif state[0] > state[1]:\n",
    "                self.running_total.append((self.running_total[-1] + 1))\n",
    "                return 1\n",
    "            elif state[0] < state[1]:\n",
    "                self.running_total.append((self.running_total[-1] - 1))\n",
    "                return -1\n",
    "        return 0\n",
    "\n",
    "    def value_function(self):\n",
    "        '''\n",
    "        Return expected reward for each state (agent sum, dealer card)\n",
    "        '''\n",
    "        \n",
    "        value_function = np.zeros((21,10))\n",
    "        for i in range(len(self.values)):\n",
    "            for j in range(len(self.values[i])):\n",
    "                value_function[i][j] = np.max(self.values[i][j])\n",
    "        return value_function\n",
    "    \n",
    "    def testing_run(self, num_episodes):\n",
    "        self.epsilon = 0\n",
    "        self.running_total = [0]\n",
    "        for i in range(num_episodes):\n",
    "            self.stick = False\n",
    "            state = [deal_card(True), deal_card(True)]\n",
    "            while not self.is_terminal(state):\n",
    "                action = self.choose_action(state)\n",
    "                reward = self.step(state, action)\n",
    "    \n",
    "    def plot_returns(self):\n",
    "        episodes = np.array([i for i in range(1, len(self.running_total))])\n",
    "        moving_avg = np.array(self.running_total[1:]) / episodes\n",
    "        plt.plot(episodes[1000:], moving_avg[1000:])\n",
    "        plt.xlabel(\"Number of Episodes\")\n",
    "        plt.ylabel(\"Winning Percentage\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_value_function(self):\n",
    "        '''\n",
    "        plotting function from: https://github.com/mari-linhares/easy21/blob/master/easy21.ipynb\n",
    "        '''\n",
    "        fig = plt.figure(\"Value Function\", figsize=(20, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        min_x = 1\n",
    "        max_x = 10\n",
    "        min_y = 1\n",
    "        max_y = 21\n",
    "\n",
    "        x_range = np.arange(min_x, max_x)\n",
    "        y_range = np.arange(min_y, max_y)\n",
    "\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "    \n",
    "        def get_stat_val(x, y):\n",
    "            return self.value_function()[y, x]\n",
    "        \n",
    "        Z = get_stat_val(X, Y)\n",
    "\n",
    "        ax.set_xlabel('Dealer Showing')\n",
    "        ax.set_ylabel('Player Sum')\n",
    "        ax.set_zlabel('Value')\n",
    "        return ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, \n",
    "                                   linewidth=0, antialiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_card(is_first):\n",
    "    '''\n",
    "    First card for dealer and agent is black (positive value),\n",
    "    for subsequent cards prob(black) = 2/3, prob(red) = 1/3\n",
    "    '''\n",
    "    value = np.random.randint(1,11)\n",
    "    if is_first:\n",
    "        return value\n",
    "    else:\n",
    "        color = np.random.random()\n",
    "        if color < .33:\n",
    "            value = 0 - value\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Agent\n",
    "Monte Carlo learning is a type of model-free learning. That is to say, our Monte Carlo agent does not attempt to learn anything about the probability of transitioning between pairs of states. It simply tracks its average reward received from each action sequence. An action sequence here is a start state, an action (hit or stick), a reward, and the resulting next state. The Monte Carlo agent updates its value function with the following:  \n",
    "$Q(s,a) = avgreturn(s,a)$  \n",
    "This is plain and simple learning from experience - over time the agent will learn whether hitting or sticking tends to yield higher reward for each state (player sum, dealer showing). Since Easy 21 involves a good bit of chance, both hitting and sticking may have negative average reward (picture yourself dealt a 14 in blackjack with the dealer showing an ace) so the agent will pick the most positive or least negative action in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloAgent(Agent):\n",
    "    '''\n",
    "    params:\n",
    "            epsilon - likelihood that agent chooses random action (hit/stick) rather than action dictated by policy\n",
    "    '''\n",
    "    def __init__(self, epsilon):\n",
    "        super().__init__(epsilon)\n",
    "    \n",
    "    def training_run(self, num_episodes):\n",
    "        '''\n",
    "        ε starts high to encourage exploration, steadily decreases as agent \n",
    "        learns better value approximations.  Policy starts as \"hit\" in every state, \n",
    "        becomes ε-greedy w.r.t. value function.\n",
    "        '''\n",
    "        returns = [[[[],[]] for i in range(10)] for i in range(21)]\n",
    "\n",
    "        for i in range(num_episodes):\n",
    "            self.stick = False\n",
    "            self.epsilon *= .9999\n",
    "            G = 0\n",
    "            state = [deal_card(True), deal_card(True)]\n",
    "            while not self.is_terminal(state):\n",
    "                last_state = deepcopy(state)\n",
    "                action = self.choose_action(state)\n",
    "                reward = self.step(state, action)\n",
    "                G += reward\n",
    "                returns[last_state[0] - 1][last_state[1] - 1][action].append(G)\n",
    "\n",
    "                self.values[last_state[0] - 1][last_state[1] - 1][action] = \\\n",
    "                    np.average(returns[last_state[0] - 1][last_state[1] - 1][action])\n",
    "\n",
    "                self.policy[last_state[0] - 1][last_state[1] - 1] = \\\n",
    "                    np.argmax(self.values[last_state[0] - 1][last_state[1] - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run\n",
    "We start with a very high epsilon to encourage exploration, and gradually move increase the frequency with which our agent chooses the \"best\" action throughout the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent = MonteCarloAgent(.9999)\n",
    "mc_agent.training_run(200000)\n",
    "mc_agent.plot_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent.testing_run(100000)\n",
    "mc_agent.plot_returns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA Agent\n",
    "SARSA stands for State-Action-Reward-State-Action, and is a form of Temporal Difference learning. Rather than waiting to receive the actual reward like the Monte Carlo agent, our SARSA agent will update its belief about its expected reward at each step. The update SARSA makes at each step is:  \n",
    "$Q(s_t, a_t) = Q(s_t, a_t) + \\alpha(r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$  \n",
    "Where $Q(s_t, a_t)$ is the value function for the state-action pair $(s_t, a_t)$, $\\alpha$ is the learning rate, or how heavily to value new information, and $\\gamma$ is the discount rate, or how heavily to weight future vs. current rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA_Agent(Agent):\n",
    "    '''\n",
    "    params:\n",
    "            alpha - agent's learning rate\n",
    "            gamma - agent's discount factor\n",
    "    '''\n",
    "    def __init__(self, epsilon, alpha, gamma):\n",
    "        super().__init__(epsilon)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def training_run(self, num_episodes):\n",
    "        '''\n",
    "        Action/value approximation: Q(s,a) = Q(s,a) + α(reward + γQ(s',a') - Q(s,a))\n",
    "        '''\n",
    "        for i in range(num_episodes):\n",
    "            self.stick = False\n",
    "            state = [deal_card(True), deal_card(True)]\n",
    "            action = self.choose_action(state)\n",
    "            while not self.is_terminal(state):\n",
    "                last_state = deepcopy(state)\n",
    "                last_action = action\n",
    "                reward = self.step(state, action)\n",
    "\n",
    "                if not self.is_terminal(state):\n",
    "                    action = self.choose_action(state)\n",
    "                    self.values[last_state[0] - 1][last_state[1] - 1][last_action] += \\\n",
    "                        (self.alpha * (reward + (self.gamma * \\\n",
    "                        self.values[state[0] - 1][state[1] - 1][action]) - \\\n",
    "                        self.values[last_state[0] - 1][last_state[1] - 1][last_action]))\n",
    "                else:\n",
    "                    self.values[last_state[0] - 1][last_state[1] - 1][last_action] += \\\n",
    "                        (self.alpha * (reward - \\\n",
    "                        self.values[last_state[0] - 1][last_state[1] - 1][last_action]))\n",
    "\n",
    "                self.policy[last_state[0] - 1][last_state[1] - 1] = \\\n",
    "                    np.argmax(self.values[last_state[0] - 1][last_state[1] - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run\n",
    "Our SARSA agent begins with a policy to hit in all states, and eventually updates its policy according to the value function. Like the Monte Carlo agent, the SARSA agent starts out quite poorly but begins to win more often quickly. The SARSA agent doesn't achieve quite the same performance overall as the Monte Carlo agent, but it learns much quicker as evidenced by the steep upwards slope very early in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent = SARSA_Agent(.9999, .1, .9)\n",
    "sarsa_agent.training_run(200000)\n",
    "sarsa_agent.plot_value_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_agent.testing_run(100000)\n",
    "sarsa_agent.plot_returns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error of SARSA Agent Value Function\n",
    "Why did the SARSA agent not perform as well as the Monte Carlo agent? Likely because the Monte Carlo agent was basing its updates on the actual results, while the SARSA agent was keeping an approximation. This allowed the SARSA agent to learn quicker, but prevented from achieving the same level of performance. We can compute the Mean-Squared Error of the SARSA agent (taking the Monte Carlo agent's value function as ground truth) with the following formula:  \n",
    "$MSE = \\Sigma((Q(s,a)_{SARSA} - Q(s,a)_{Monte Carlo})^2$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_MSE(mc_agent, sarsa_agent):\n",
    "    return np.sum((mc_agent.values - sarsa_agent.values)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_MSE(mc_agent, sarsa_agent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
